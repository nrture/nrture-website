How I debugged and fixed Interaction to Next Paint on my website
Hi everyone

Recently I was able to reduce my INP from 300ms to 160ms after struggling to fix it for 3 months! Here's a summary of how I did it in case it's helpful for any of you (I also wrote a post with the fulls details):

Firstly, my web site is built with Nextjs / React

Jacob GroÃŸ, a senior performance engineer at Framer, has a great post on how to improve INP in React. My two biggest takeaways:

Use concurrent mode features like startTransition and useDeferedvalue to schedule changes as non-urgent and non-blocking

Mark components as non-urgent by using "selective hydration" i.e. suspense boundaries (or dynamic imports if you're using Next.js)

I implemented his recommdations but still my INP was 260ms

Then I tried 3 different tools to monitor INP:

DebugBear: It's great but way too expensive

Vercel Speed Insights: A nice UI but doesn't give enough information

Eventually I just implemented Google's web-vitals library and logged metrics myself. I combined this with session replays so I could actually see what people were doing when the bad INP was logged ðŸ¤¯

I found the problematic component (a menu dropdown). Two issues I noticed:


After using the React Profiler and enabling "highlight updates when components render", I saw the menu items were constantly re-rendering when scrolling. To fix this I memoize the menu items

2. Opening the menu was causing a spike on mobile devices. To fix this I used a combination of state to open the menu and `startTransition()` so that the keyboard has a chance to open first before the menu

That's it!
Here are some other posts I read that helped me figure this all out:

Optimizing INP for React apps by TK Kinoshita

Vercel's guide on optimizing core web vitals

Web.dev's guide on how to optimize INP



[MUSIC PLAYING] SPEAKER: Today, I want to take a look at how to diagnose and fix Interaction and Next Paint issues using the DevTools Performance panel. Now, there are a lot of ways to approach debugging INP problems like setting up your Analytics so you know where your real users are having performance issues. But there are times when you don't have that luxury. Sometimes all you've been told is that a particular page is having a problem and you need to fix it. Remember that INP measures the time from when you interact to when the browser is able to paint a response on-screen. I type in the search box, and ugh. It takes a long time for me to see if the page even knows I did something. My code and the things my code is causing the browser to do are getting between interaction and response. At a high level, then, the ways we fix that are to take that code and ditch the unnecessary parts. For any part of the code that doesn't need to run before next paint, defer it. Run it in the next frame or when that main thread has calmed down. And, finally, for the remaining code that we've decided absolutely has to run before the next paint, we optimize it. Our job is to keep the page responsive. As we run through problems, identify which of these buckets apply so we can get out of the way of our users actually doing whatever it is that they need to do on the page. So start by opening DevTools like this either by right-clicking and inspecting or with a keyboard shortcut. For most pages, a poor INP experience is most common on a mobile device, so we'll start there. Turn on mobile device emulation. So we don't know exactly what the issue is. I'll start searching for slow interactions by interacting with the page as a user would. While doing that, we'll take a performance recording to measure what's slow and needs attention. Switch to the performance panel. Now here's a vital step. I'm running this page on a desktop machine on Wi-Fi, but I want to know what it's like on a slower mobile device. At the top of the window, there's some helpful mobile throttling presets. I usually pick the mid-tier mobile option. If you click that gear that's not red, you can see the throttling that was actually applied. Most relevant for INP, there's a 4x CPU slowdown. Possibly less relevant, it's also going to emulate a fast 3G network connection. You might see the same preset called Slow 4G in tools like Lighthouse, but it's the same thing. If you typically have users on really slow devices, or you have a speedy development machine, consider using the even slower 6x CPU slowdown to really get a feel for what could go wrong. All right, we're set up. Let's get started. A couple of possible interactions stand out. When we switched to a mobile view, the side nav became a hamburger menu. Mobile-only UI like that can be easy to miss when testing on a desktop. Then there's the big search box at the top. So hit the Record button, open and then close the side nav, and type a few letters into the search box. And stop recording. This is a timeline of all the page's activity while we interacted with the page. There's a ton of things visualized here, but we don't need to know the meaning of everything on-screen in order to start finding slow interactions and what caused them. We just need entry points places to start looking. First, you'll see up here at the top as we mouse over, these are screenshots of what the page looked like throughout the recording. The page start's already loaded and going along. You can see where we click the hamburger button and open the side nav. Then, here, we started searching. Each of the labeled horizontal tracks below this are aligned with the same timeline, each giving a different view of what occurred during the recording. To get our INP bearings straight, start with the Interactions track. Click to expand it. As you can see, we have a number of interactions. The first thing that stands out is this monster keyboard event from when I started searching. That's definitely a top priority. Over here is a pointer interaction from when I open the side nav. Pointer interactions encapsulate clicks and taps depending on the device. This one is pretty slow, so it's a good one to check up on. There's also the shorter click from closing the side nav that's slower than I'd like, but fast enough for now, so we won't look more into it today. So let's take a look at this side nav one. I'll move it to the center of the panel, either first-person shooter style by hitting A or D, or I can click and drag. Then we want to zoom in on it by holding W or scrolling with the mouse wheel or trackpad. If you haven't used the Performance panel much, take some time to slide around and zoom in and out to get used to it. Hovering over a key pointer interaction, you get a few pieces of information. The entire thing took 246 milliseconds-- not a good INP. On a slower device, or if the CPU happens to be busy doing other things, it could easily be worse. There's a further breakdown of the duration in this tooltip. You can see that there was 35 milliseconds of input delay, which is the time from the actual click to when the event listener started acting on it. This is represented by the whisker on the left of the interaction block. Then 116 milliseconds of processing time, that's our actual JavaScript handling the event and any knock-on effects it might have had like triggering layout it's represented by the center block of this interaction. Finally, after our JavaScript is finished running, there's another 95 milliseconds of presentation delay, which is the time from when our JavaScript finishes to when the browser is actually ready to show any changes on-screen-- in this case, when the side nav starts opening. It's represented by this long right whisker. The interactions track lines up vertically with the main thread track below it, which shows what was going on in the main thread while we were interacting. If you're familiar with long tasks, you can see that there were actually multiple tasks during this interaction-- not just a single one. These have been highlighted as particularly long tasks likely to affect the responsiveness of the page. If you click on the task and look at the Summary pane at the bottom, it shows you where all that time went-- mostly JavaScript and rendering, which is style and layout, in this case. Immediately below the task blocks is the root of most this time, the thing that ended up calling everything else in the stack-- the click event and JavaScript. And that's probably our event listeners. How this time is actually spent, however, is laid out at the bottom of these stacks and is split up into what looks like two types of execution. There's a stack of JavaScript function calls responding to the click event and culminating in a large block of recalculate style. Something triggered some DOM or style change that caused a ton of browser work to recalculate the styles and layout for potentially all the elements on-screen. So that's immediately suspicious to me because, if you remember the actual interaction, it's just sliding the slide nav out. The majority of the page shouldn't be affected by that, so there shouldn't be any need for a huge style recalc. So maybe there's something going on that's unnecessary. A lot of times when you're diagnosing a live site, you'll mostly have anonymous functions like this or obfuscated names like these functions-- o and e. When debugging performance, you want to debug a site that's identical to what your users are seeing-- not a site with a debug build, which can have very different performance behavior. One way of dealing with that obfuscation is to build source maps. But if for some reason you can't ship source maps, you can often still click through and see what's going on at a high level so you know enough about what to change in the original code. That's what we'll demonstrate here-- find that low-hanging performance fruit without directly seeing the original code. So click on a function entry and see information about it in the Summary pane. You can see the Total Time and the Self Time. Most of these functions spent very little time executing code in themselves-- which is their self time-- but they have a long total time due to the functions that they subsequently called. Most importantly in the Summary pane, we get a script position for this function in our JavaScript bundle. If you click on this, we're taken over to the Sources panel where we can see the function body. So it's interesting that I see entries like this-- watchforInert and setInert-- which doesn't ring a bell for code that I wrote for the site. Let's keep that in mind but set it aside for the moment. If we go back to the Performance panel, we can now look directly at that recalculate style where most of the execution time was spent. Something forced our DOM to update. And when you select it, you can actually see that there's a helpful connector from the recalculate style back to what caused the recalculation. It's a teeny tiny block that's difficult to click, but hopefully you can click on this initiated by entry in the Summary pane. And DevTools will select the root cause for you. Now, ideally, at this point, I could click the JavaScript source position and get to the function that caused this, but it doesn't always work out. You'll be taken to a code that was likely generated by your build process. If that happens, though, what I find often works is to take a single step up the flame chart stack and usually that source position will, indeed, lead me to the actual root cause. So here it is, which is setting inert on the main element. So now that watchforInert code from earlier makes sense. It's not code that I wrote, but I was looking at this earlier. And when I searched through my node modules-- or I could have searched on GitHub-- I found that function name is from code from an inert polyfill. But the mystery is why a polyfill is taking so much time in a browser that supports inert. Switching over to my code-- let's see-- we find this polyfill is included by a single line. I believe I added it in a time when inert wasn't widely supported across browsers yet, so I added the polyfill to make sure that all users got something that mimicked the true inert behavior. So looking into the problem, it turns out I happened to use the exact version of this polyfill with a bug that caused it to always run even when there was built-in support. I could update the polyfill to a later version that will fix the bug, but checking Can I Use today, inert is now widely available across browsers. So the polyfill isn't needed at all anymore. I'll just delete that line and not ship extra code to my users. Let's rebuild and reload. Start recording, open the side nav, and now take another look. Here's the difference that this made. The processing time of this pointer interaction dropped from 113 milliseconds to 43. The recalculate style block has improved from 73 milliseconds to 38 here. It's still changing the style for a majority of the elements currently in the DOM nested in the main element, but now it's using the built-in inert instead of a bespoke set of styles from the polyfill. Most importantly, for the change we made, though, you can see that there's no longer any JavaScript stack visible. It's almost entirely a style recalc based on a click event. And if we look for the initiator, and then click on its parent JavaScript to reveal the code-- yep-- we can see that it's still setting inert. There's just no longer a bunch of JavaScript triggered by this line. At this point, we can see that on a mobile device or a slow device, there's still a real cost to setting inert. And in a situation like this, it's up to you to evaluate the trade-offs of that extra 38 milliseconds of recalculate style. Maybe the design and focus handling could be tweaked to work well without inert. Another option would be to defer setting inert maybe to after the slide-out animation is complete. Since it's the vast majority of the processing time in this interaction, I'm going to call 38 milliseconds good enough for now and leave that to a possible future optimization. So let's look at this interaction, overall, again. The click response is still relatively slow at 169 milliseconds. Technically, it qualifies as good INP, but no one would call it great. We have the input delay of 25 milliseconds. In the last trace, it was 35. And we didn't do anything to improve that, so we can maybe assume that level of variance is normal and it could easily go the other way at any time. It's not clear from the main thread activity what we could do to improve input delay at this point, though, so let's move on. Processing time is down to 43 milliseconds, but we still have this presentation delay of 101 milliseconds. What's going on to prevent that first frame of the slide-out animation from immediately being painted to the screen at this point? A lot of this time is pre-paint, paint, layerize in the main thread. And we're not going to get too much into this today. But down in the GPU process, you can see a good bit of work preparing to get this on-screen. So something is slow. Something is causing a lot of paint. And looking at the actual content, you may be able to guess what it is. The side nav slides out, but that's using CSS transforms in a transition, so I'm not worried about that. Styling-wise, the side nav has a box shadow, and the background is blurred. So you may have heard this before or you're hearing it now, blurring is actually a relatively expensive GPU operation. Content is painted, but then the pixel data is read back in, and the new painted values come from smoothing a neighborhood of surrounding pixels for each pixel. This 101 milliseconds is also on a desktop machine. Even with CPU throttling on, it's probably going to be a bigger hit on a real mid-tier mobile device. These paint-related tasks out here are from the slide-out animation, and I would bet that they're longer than they would be without the blur, preventing us from hitting 60 frames per second in this animation. So, again, we need to evaluate, is this something worth the performance trade-off? A blurred background isn't necessary to communicate to the user that they're tap-registered and the menu is appearing, so let's get rid of it. There are probably ways to optimize the effect. Maybe the perf is worsened due to interaction with the inert layout changes, for instance. But for now, let's just call it unnecessary and remove it to bring up performance. Switching back over to the code, let's find where that blur is. I have no idea, so let's just [CHUCKLES] search for it. And, hey, lo and behold, we only have one blur, and it's very obviously set on the page when main is set to inert. This time, we'll just get rid of that, rebuild, and see what happens. Record opening to the side nav, and with this interaction, we have essentially the same input delay and processing time, but the presentation delay has been brought down from 101 milliseconds to 8. The long paint task we saw before is now basically gone. The GPU activity is gone. These animation tasks are a little slow but well within the frame budget now. And the browser is now able to go quickly from the inert-style recalc to the first frame of the nav slide-out painted to the screen. Stepping back, we've reduced our interaction from 246 milliseconds when we started down to 74 milliseconds with 4x CPU scaling on. If I wanted to optimize further, I could look into where this input delay is coming from or look into moving that time spent recalculating inert. Now that the interaction is relatively fast, I could also look into adding performance styling back to the side nav if I wanted to indicate that the main content is inert in a different way, if that seemed necessary. But, right now, I'm thinking back to that first trace and those monster keyboard interactions. I needed to address those first. So, as before, start recording, but now take a look at the text box and type O, U, S. And you can really feel that delay. Finally, I'll type O-U-S quickly, and it's [LAUGHS] so slow that they appear all at once. All right, here you can see our keyboard interactions, and it's a bit of a mess. Type in O, 713 milliseconds. Type in U, 264 milliseconds. Type in S, and it's actually in two pieces here, which is interesting. These interactions are getting shorter as we add more letters, which makes sense because the search is filtering out entries and removing them from display. There are fewer terms that have an O, even fewer that have an OU, and just a few containing OUS. Looking at this first keyboard interaction, starting at the top of the stack, there are two major tasks, and each corresponds to what look like two different keyboard event listeners. A keypress event turns to text input turns to input, so I assume that everything under this is the result of an input listener. And then we have a different event-- the keyup-- with a lot of JavaScript underneath it. And then two of these get grouped into one keyboard interaction. All right, so the two listeners won't necessarily be part of the same interaction, though. Remember that INP is Interaction to Next Paint. If I press a key down and then hold it there, the browser may be able to paint the result of the interaction before I lift my finger. And that's actually what's happening over here when I typed the S. I didn't hold the S key for an unusual amount of time, but the input listener was able to complete quickly enough-- 62 milliseconds-- that the browser was able to paint a frame before I managed to release the key. That's why this one keypress is split into two interactions, and it tells us what's happening over here in these much longer keyboard interactions. My input listener is taking so long on keypress that it's still running by the time I lifted my finger and the key up "could" fire, so my keyup listener had to wait. That's input delay for the keyup, but DevTools has helpfully merged them together to show that, functionally, it's a single giant blocking interaction. All right, this is taken to the absurd extreme when I typed O-U-S quickly. Now all three keyboard interactions, each with two event listeners each, overlap enough that they block the next frame for 1.2 seconds. This is the danger of long keypress listeners. I wasn't typing that fast, so just regular typing is enough to make the page fully unresponsive as long as the typing continues and then the queue of keyboard listeners slowly catches back up. All right, let's isolate a single keypress to see what's going on. First, there's this run of JavaScript triggering a bunch of forced layout, which we'll get to. But I wanted to zoom in on the keyup listener first. Near the top of the stack, we see "push"-- which might ring a bell to some-- and then an inverted mountain of minified JavaScript that doesn't look like fun to wade through at all. So, instead, let's head back up to my code that calls all this. We can see that on keyup. There it is. I'm taking the search value and pushing to data layer. This is a custom event, a trigger to send some kind of data to a Tag Manager, which then spends 100 milliseconds or so processing it doing whatever with that text. But it appears to have no effect on the presentation of the search term on-screen. It's processing data, whether that's some sort of analytics, or advertising, or whatever. So if it's not necessary for the next paint, that means that this is 115 milliseconds that can happen after the next paint. So let's see what we can do to delay a little bit and get it out of the way of the browser's response. We're looking for a keyup event, so let's just search for that. And yeah. On keyup, just like we saw, it takes the search value and pushes to trigger processing of it. So let's defer that. We could reach for something like requestIdleCallback, but this time I'll take the simpler approach with a set timeout. Now how long should we defer? A timeout of 0 would try to run it as soon as possible. But as we've seen, that could then be interfering with other keypresses and start a domino effect of each interaction delaying the processing of the next by an ever-increasing amount. Let's defer it out to a point that if there hasn't been another keypress, more probably aren't coming. Let's say, one second. Another issue is that with every keyup, you'll get another set timeout, potentially setting up a whole lot of long tasks starting at one second from the first keyup. Since dataLayer.push overwrites anything that's already been pushed anyways, we may only care about the most recent value, so we can do a debounce. A really light way to do that is just keep a set timeout ID and save it from our set timeout. And then on every keyup, we'll just unconditionally clear any set timeout that may already be set, because we don't care about the old one anymore-- just the latest one. Let's take a look at the result. Reload, type O, U, S, and then give it at least one second before stopping. The first keyboard interaction-- the O-- is now 616 milliseconds, 100 milliseconds faster than before. It's all input listener. But if you zoom way in, you'll see the keyup is still there, just a few microseconds long now. And the only thing, really, in there is that set timeout. And then way out here, past the other keyboard interactions, you can see we now have that single deferred and debounce trigger of our custom event. It can then do its processing. Looking at this task now, I would say, hey, we have this long task. It's 88 milliseconds. I need to go into my Tag Manager and audit the kinds of things that are in there, remove things that aren't needed, and defer things that don't need to happen right away, because 90 to 100 milliseconds needs to be broken up. You never know when some other interaction will occur like, I opened the hamburger menu after typing. It happens to coincide with this task and, suddenly, that interaction is 90 milliseconds slower than it should have been. But Tag Manager management is out of scope of this video. And it's covered well on web.dev, so go check out the resources there. All right, we still have this incredibly long interaction to deal with. We have these functions, a and o, that are the root of the rest of all of this. First, a little JavaScript, then a relatively long recalc-style and layout-- 100 milliseconds of that-- and then there's these two interesting sections where there are all these tiny forced reflows. When it looks like this, you know it's almost definitely something in a loop forcing layout over and over again. So that's going to be a clue for what we're looking for. Click above this and go to the Sources panel. This selects all the glossary term elements, and here's the loop over them. And then what jumps out is code comparing against offset height here and setting the display style-- classic layout thrashing. Let's go find out what's causing this. All right, where do we look? Well, we know a couple of things. It's checking offset height and setting display, so let's try searching for that. Offset height, and here we go. If we needed to narrow it down more, we could have kept adding more specific terms for the minified code, but this looks like the place. Now where's this function called? And here's the loop over all the term elements that we saw, and then it looks like it's toggling display based on if the search string is present. There's also a secondary check here for currentlyVisible based on the element's offset height and not changing it if it's not needed. So, I think, looking at this code, maybe the thought behind it was that we don't want to do extra work if the element already has the display value it needs to have. But if that was the idea, it definitely backfired. What actually happens is it's looping over terms, it gets to one that needs to be hidden, and sets display-- none. That changes the layout of the page because the element has been removed from the document flow, and the browser marks the page's layout as needing an update. Then it moves to the next term and checks its offset height, but that depends on the layout of the page. [LAUGHS] The browser, then, needs to actually do the layout, find where everything fits, and then return that computed offset height. If it then proceeds to say that element is displayed in none, as well, that changes the layout of the page again and forces another round of layout in the next turn of the loop-- and again and again. This offset height check has actually created a bunch of extra work for the browser and not saved anything. And that's what's called layout thrashing, which may be a term you're familiar with. Now, you could get clever and change this code to check a value that doesn't force layout like offset height does, or you could even make a little weakmap visibility virtual DOM in, like, six lines of code. But in this case, the much simpler approach is to treat the DOM as write-only and trust the browser's performance. So let's just take the check out and set display based on includeTerm. And now it's a much simpler function, and it'll run much more quickly. Start recording. O, U, S. And you can already tell it's considerably faster. And here, we see the results. We've dropped this giant interaction from 616 milliseconds to 135. And if we look closer, that loop of layout thrashing is completely gone. I think it was in here. So what's left? We have these functions, a and o, as before. If we click through to the source and unfocus our eyes a bit, you can see that there's a loop removing all the terms from the DOM, then sorting them, then appending them again. Then there's this short section that used to be the layout thrashing and is now much shorter, looping over the terms and hiding them if needed. And this looks like it's underlining the search string in each term. Finally, there's a good 40 milliseconds recalculating style and doing the layout for this long list of terms. All right, what's the goal? Get this main thread work out of the way so the browser can paint a response. First and foolproof approach would be to defer this work. If we yield to the main thread, the browser will take care of the response by showing the typed input in the search box immediately, and then we can follow up with the task, actually sorting and filtering the terms in the DOM. Throw in a debounce, and this works right now. We can call it a day. If you don't have time for more, do this rather than waiting for the perfect solution. But for this video, I'm thinking we could get a little more involved. Importantly, only a certain number of glossary terms can appear on the screen at once. Even the tallest portrait-oriented monitor can only show so many. The browser also scrolls back to the input whenever you type in it, so we only need to worry about the top of the list. So here's the idea. What if on keypress, we only displayed in-style the top, let's say, 10 terms, then yielded to the main thread so we can get a paint, and then add and style the remaining terms after that? We won't get all of this time broken up, but we will get a lot of it, especially since the layout part will only need to run over 10 items in the first block of execution. So let's look for, say, that remove. And this is exactly what we saw on DevTools. Selects all the terms in the page, removes them, sorts them, adds them back, and then styles them. The remove loop needs to stay the same. We don't want stale terms in the page when we add new ones. Likewise, we won't touch sorting. You can't really do part of a sort, or, at least, we wouldn't generally be saving any time at all to get the top elements instead of sorting all of them. What we do want to break up is the append loop and the styling loop, so we only style and append the first 10 items, yield, and then style and append the rest. First, I'll bring the loop out of style terms so that it works over a single element instead of all of them. Hide the term if it doesn't have the search string. Then, out here, I'll combine the append and [? style ?] loops into one loop, so now this is functionally equivalent to how it was before. Now add a counter so it knows when to yield after 10 terms have been added. And, really, we want 10 visible terms, so let's add a visibility return to style term, and we'll count that. Finally, after 10 terms have been added, we yield. Some day, we'll have a real scheduling primitive like scheduler.yield everywhere, but, for now, we have to bring our own. In this case, I'm going to use the simple set timeout version wrapped in a promise for nice awaiting ergonomics. Finally, we want to make sure the collar, which is the event listener, is also async. You'll need to double-check that it works to make it async since that can change some things-- like if it needs to call preventDefault-- and you might need a different yielding strategy, like a queue of work to be done, in that case. But in this case, it's straightforward listener, so it's no issue. Let's see our finished work in action. Start typing O, U, S and stop. And we can see our keyboard interaction has moved from 713 milliseconds, originally, to 134 milliseconds, and now to 78 milliseconds. And don't forget we have CPU throttling on. And as predicted, we still have the element removes here-- the sort-- but the styling is short, and only three milliseconds of style layout before yielding. And then sometime after the next frame, we come back, and this looks like the rest of our loop. Now the rest of the style and layout, it's still significant. We've got at least 30 milliseconds here. It's well after the interaction. The browser responded. It feels good, and it only comes later. It's still a long task, though, so we might consider adding another await in that loop-- maybe after 50 items, or 200 items, half the items, whatever-- and yield the main thread one more time or a couple more times to make sure that we're not blocking for too long. But we also don't want to ruin the user experience. It needs to be balanced against how users typically interact and scroll through the page. But, I think, for now, we can call our interactions here good-- really, better than good-- good enough. The search has moved well into the good INP territory even with 4x CPU throttling. The side nav pops out without a hitch, at least on my machine. At this point, I would want to deploy my changes. In an ideal world, I'd have RUM data being collected so I could ensure that things really are better for my actual users. It's completely possible that after 28 days my new RUM or CrUX numbers come in and there's still issues from interactions I didn't think to try out here or aspects of the page that are different on real mobile devices. And I'll start this process again.